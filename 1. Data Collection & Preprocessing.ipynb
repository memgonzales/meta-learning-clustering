{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing\n",
    "\n",
    "**Mark Edward M. Gonzales, Lorene C. Uy, and Jacob Adrianne L. Sy (CSC713M)**<br>\n",
    "mark_gonzales@dlsu.edu.ph, lorene_c_uy@dlsu.edu.ph, jacob_adrianne_l_sy@dlsu.edu.ph\n",
    "\n",
    "In partial fulfillment of the requirements for the Machine Learning graduate class (CSC713M) under **Dr. Macario O. Cordel, II** of the Department of Computer Technology, College of Computer Studies, De La Salle University, this notebook details the process and presents the code for **data collection and preprocessing** stage of the investigatory project titled \"Automatic Recommendation of Distance Metric for $k$-Means Clustering: A Meta-Learning Approach.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries and modules — all of which are automatically bundled with an Anaconda installation — were used in this notebook:\n",
    "\n",
    "Library/Module | Description | License\n",
    ":-- | :-- | :--\n",
    "<a href = \"https://docs.python.org/3/library/os.html\">`os`</a> | Provides miscellaneous operating system interfaces | Python Software Foundation License\n",
    "<a href = \"https://docs.python.org/3/library/shutil.html\">`shutil`</a> | Provides high-level operations on files and collections of files | Python Software Foundation License\n",
    "<a href = \"https://pandas.pydata.org/\">`pandas`</a> | Provides functions for data analysis and manipulation\t | BSD 3-Clause \"New\" or \"Revised\" License\n",
    "<a href = \"https://numpy.org/\">`numpy`</a> | Provides a multidimensional array object, various derived objects, and an assortment of routines for fast operations on arrays | BSD 3-Clause \"New\" or \"Revised\" License\n",
    "\n",
    "*The descriptions were lifted from their respective websites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of **340 datasets** were collected from two sources:\n",
    "- **195 datasets** were taken from the collection published by Pimentel [1] in OpenML [2]. The suitability of this collection of datasets for meta-learning studies related to clustering is supported by its use in several studies [3, 4, 5, 6].\n",
    "- **60 datasets** were taken from the University of California, Irvine (UCI) Machine Learning Repository [7]. From among the datasets tagged as suitable for clustering tasks, a subset of those with ground-truth labels was chosen. The presence of ground-truth labels is necessary since one of the cluster validity indices used in this study (i.e., the adjusted Rand index) is an *external* measure of cluster quality.\n",
    "- **85 datasets** were taken from Kaggle. From among the datasets tagged as suitable for multi-class or binary classification, a subset of those with ground-truth labels was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data wrangling process starts with a manual inspection of the datasets. In particular,\n",
    "1. The string `_noheader` is appended to the filenames of those without header rows. \n",
    "2. For uniformity, some of the files are restructured such that <br>\n",
    "   a. The ground-truth labels are found at the last column <br>\n",
    "   b. The categorical features are at consecutive columns after the numerical features.\n",
    "3. All the datasets are converted to CSV (comma-separated values) format.\n",
    "\n",
    "The datasets at the end of this stage of manual data wrangling can be accessed through these links: \n",
    "- [OpenML Datasets](https://drive.google.com/drive/folders/1CxUsyiKmCpgNlS9pePh_tXjsrhgzHb_U?usp=sharing) <br>\n",
    "- [UCI Machine Learning Repository Datasets](https://drive.google.com/drive/folders/1kU2SHzluFAwNG3sMdBLT-0D9RFtcwq88?usp=sharing)\n",
    "- [Kaggle Datasets](https://drive.google.com/drive/folders/17VyMgsNNayMv0yc1X86MkaxrLrPUK8QN?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since some of the datasets, namely the *(i)* Gas Sensor Array Drift Dataset and *(ii)* the User Identification From Walking Activity Dataset, are distributed across multiple files, scripts are written to consolidate them into single CSV files. These scripts assume that the files are stored inside the directory `datasets/for_cleaning`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gas Sensor Array Drift Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below converts the [DAT files](https://drive.google.com/drive/folders/1SQHuLomJePQ6N8kGX84SYlOaS3-2aYtn?usp=sharing) into CSV files and consolidates them into a single CSV file. It assumes that the files are stored inside the directory `datasets/for_cleaning/gas_sensor` and that the resulting CSV file will be stored in the directory `datasets/uci_datasets` with the filename `gas_sensor_noheader.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1, 11):\n",
    "    f = None\n",
    "    \n",
    "    with open(f'datasets/for_cleaning/gas_sensor/batch{j}.dat', 'r') as file:\n",
    "        f = file.read()\n",
    "        f = f.replace('\\n', str(j) + '\\n')\n",
    "        \n",
    "        # The entries are stored in the .dat file in the form x:y or x;y, where x acts as an index.\n",
    "        # Therefore, in the conversion to CSV file, x: and x; are removed.\n",
    "        for i in range(129, 0, -1):\n",
    "            f = f.replace(str(i) + ';', '')\n",
    "            f = f.replace(str(i) + ':', '')\n",
    "            f = f.replace(' ', ',')\n",
    "            \n",
    "    with open(f'datasets/for_cleaning/gas_sensor/batch{j}_noheader.csv', 'w') as file:\n",
    "        file.write(f)\n",
    "        \n",
    "src_files = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    src_files.append(f'datasets/for_cleaning/gas_sensor/batch{i}_noheader.csv')\n",
    "    \n",
    "with open(f'datasets/uci_datasets/gas_sensor_noheader.csv', 'w') as dest_file:\n",
    "    for src_file in src_files:\n",
    "        with open(src_file) as file:\n",
    "            for line in file:\n",
    "                dest_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Identification From Walking Activity Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below consolidates the [CSV files](https://drive.google.com/drive/folders/1amW3Y8XrWwMfqYrsygcAAsnGyM69G3ct?usp=sharing) into a single CSV file. It assumes that the files are stored inside the directory `datasets/for_cleaning/user_walk` and that the resulting CSV file will be stored in the directory `datasets/uci_datasets` with the filename `user_walk_noheader.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1, 23):\n",
    "    f = None\n",
    "    \n",
    "    with open(f'datasets/for_cleaning/user_walk/{j}.csv', 'r') as file:\n",
    "        f = file.read()\n",
    "        f = f.replace('\\n', ',' + str(j) + '\\n')\n",
    "            \n",
    "    with open(f'datasets/for_cleaning/user_walk/user_walk{j}_noheader.csv', 'w') as file:\n",
    "        file.write(f)\n",
    "        \n",
    "src_files = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    src_files.append(f'datasets/for_cleaning/user_walk/user_walk{i}_noheader.csv')\n",
    "    \n",
    "with open(f'datasets/uci_datasets/user_walk_noheader.csv', 'w') as dest_file:\n",
    "    for src_file in src_files:\n",
    "        with open(src_file) as file:\n",
    "            for line in file:\n",
    "                dest_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART IV: One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is performed to convert categorical features into a form that can be fed into machine learning classifiers,.\n",
    "\n",
    "The function below creates a CSV file after performing one-hot encoding on the categorical features of the given dataset.\n",
    "\n",
    "**Parameter**:\n",
    "- `filename`: Filename (together with the file extension) of the dataset\n",
    "- `num_categorical`: Number of categorical features in the dataset\n",
    "- `output`: Filename of the output CSV file\n",
    "\n",
    "**Preconditions**:\n",
    "- The ground-truth labels are found at the last column\n",
    "- The categorical features are at consecutive columns after the numerical features. This implies that the last `num_categorical` columns (before the last column, which is reserved for the ground-truth labels) correspond to the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_orig_to_one_hot(filename, num_categorical, output):\n",
    "    has_header = \"noheader\" not in filename\n",
    "    data_df = pd.read_csv(filename, header=None if not has_header else 'infer')\n",
    "    data_header = data_df.columns.values\n",
    "\n",
    "    # Assumes that the label is at the last column and that the categorical features \n",
    "    # are at consecutive columns after the numerical features.\n",
    "    start_col = len(data_header) - num_categorical - 1\n",
    "    end_col = len(data_header) - 1\n",
    "\n",
    "    non_categorical = data_df.iloc[:, 0:start_col]\n",
    "    categorical = data_df.iloc[:, start_col:end_col]\n",
    "    label = data_df.iloc[:, end_col:]\n",
    "\n",
    "    categorical_headers = categorical.columns.values\n",
    "\n",
    "    # One-hot encoding for categorical values.\n",
    "    one_hot = pd.get_dummies(categorical, columns=categorical_headers)\n",
    "        \n",
    "    # Insert the one hot encoded categorical values into the categorical data's position.\n",
    "    new_df = pd.concat([non_categorical, one_hot, label], axis=1)\n",
    "\n",
    "    # Save data to CSV.\n",
    "    new_df.to_csv(output, index=False, header=has_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a manually constructed dictionary that stores the number of categorical features in the datasets with categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets to be one hot encoded\n",
    "datasets_to_be_processed = {\n",
    "    \"analcatdata_apnea1\":\t3,\n",
    "    \"analcatdata_apnea2\":\t3,\n",
    "    \"analcatdata_apnea3\":\t3,\n",
    "    \"analcatdata_boxing1\":\t3,\n",
    "    \"analcatdata_boxing2\":\t3,\n",
    "    \"analcatdata_chlamydia\":\t3,\n",
    "    \"analcatdata_creditscore\":\t2,\n",
    "    \"analcatdata_germangss\":\t5,\n",
    "    \"analcatdata_impeach\":\t10,\n",
    "    \"analcatdata_lawsuit\":\t1,\n",
    "    \"analcatdata_michiganacc\":\t2,\n",
    "    \"analcatdata_neavote\":\t3,\n",
    "    \"analcatdata_seropositive\":\t1,\n",
    "    \"analcatdata_vineyard\":\t2,\n",
    "    \"analcatdata_wildcat_noheader\":\t2,\n",
    "    \"backache\":\t26,\n",
    "    \"badges2\":\t3,\n",
    "    \"calendarDOW\":\t20,\n",
    "    \"cars1\":\t2,\n",
    "    \"chscase_vine2\":\t2,\n",
    "    \"cleve\":\t7,\n",
    "    \"cloud\":\t1,\n",
    "    \"cpu\":\t1,\n",
    "    \"dataset_10_lymph\":\t15,\n",
    "    \"dataset_48_tae\":\t4,\n",
    "    \"dataset_106_molecular\":\t57,\n",
    "    \"flags\":\t26,\n",
    "    \"fruitfly\":\t2,\n",
    "    \"grub\":\t8,\n",
    "    \"hayes\":\t4,\n",
    "    \"lowbwt\":\t7,\n",
    "    \"mu284\":\t6,\n",
    "    \"openml_phpZNNasq\":\t15,\n",
    "    \"php7gmqTJ\":\t5,\n",
    "    \"phpJ1rDu3\":\t5,\n",
    "    \"phpnYQXoc\":\t7,\n",
    "    \"phpO72JYX\":\t6,\n",
    "    \"phpOAYun7\":\t5,\n",
    "    \"phppZkQRw\":\t8,\n",
    "    \"phpRql5hp\":\t4,\n",
    "    \"phpSj3fWL\":\t7,\n",
    "    \"phpSNaed2\":\t9,\n",
    "    \"phpswpP3r\":\t1,\n",
    "    \"phpTXWrKb\":\t4,\n",
    "    \"phpXxoe1Q\":\t22,\n",
    "    \"plasma_retinol\":\t3,\n",
    "    \"PopularKids\":\t9,\n",
    "    \"pwLinear\":\t10,\n",
    "    \"servo\":\t4,\n",
    "    \"sleuth_case2002\":\t6,\n",
    "    \"solar-raw\":\t12,\n",
    "    \"teachingAssistant\":\t4,\n",
    "    \"transplant\":\t1,\n",
    "    \"veteran\":\t4,\n",
    "    \"vinnie\":\t2,\n",
    "    \"visualizing_livestock\":\t2,\n",
    "    \"zoo\":\t15,\n",
    "    \"sobar-72\":\t19,\n",
    "    \"online_shoppers_intention\":\t10,\n",
    "    \"dress\":\t10,\n",
    "    \"obesity\":\t13,\n",
    "    \"hcvdat0\":\t1,\n",
    "    \"heart_failure_clinical_records_dataset\":\t5,\n",
    "    \"Data_Cortex_Nuclear\":\t3,\n",
    "    \"nonverbal_tourists\":\t21\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below iterates through all the pertinent datasets to perform one-hot encoding. It also assumes that the datasets are stored in two folders `openml_datasets`, `uci_datasets`, and `kaggle_datasets` (depending on their source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()\n",
    "dataset_folders = ['openml_datasets', 'uci_datasets', 'kaggle_datasets']\n",
    "\n",
    "for folder in dataset_folders:\n",
    "    # get datasets \n",
    "    datasets = os.listdir(f'./{folder}')\n",
    "    # mkdir for one hot datasets\n",
    "    one_hot_folder = f\"one_hot_{folder}\"\n",
    "\n",
    "    if(os.path.isdir(one_hot_folder)):\n",
    "        shutil.rmtree(f\"./{one_hot_folder}\")\n",
    "    os.mkdir(f'./{one_hot_folder}')\n",
    "\n",
    "    for dataset in datasets:\n",
    "        filename, ext = dataset.rsplit('.', 1)\n",
    "        num_categorical = datasets_to_be_processed.get(filename)\n",
    "        if(num_categorical is not None):\n",
    "            convert_orig_to_one_hot(f\"{directory}/{folder}/{dataset}\", num_categorical, f\"{directory}/{one_hot_folder}/{filename}_one_hot.{ext}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lastly, all 340 preprocessed datasets are transferred to the directory [`final_datasets`](https://github.com/memgonzales/meta-learning-clustering/tree/master/final_datasets).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] B. Pimentel, \"Datasets,\" *OpenML*, 2017. [Online]. Available: https://www.openml.org/s/88/data.\n",
    "[Accessed: Apr. 2, 2022]\n",
    "\n",
    "[2] J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo, \"OpenML: Networked science in machine\n",
    "learning,\" *ACM SIGKDD Explorations Newsletter*, vol. 15, no. 2, pp. 49-60, June 2014.\n",
    "\n",
    "[3] X. Zhu, Y. Li, J. Wang, T. Zheng, and J. Fu, \"Automatic recommendation of a distance measure for\n",
    "clustering algorithms,\" *ACM Transactions on Knowledge Discovery from Data*, vol. 15, no. 1, pp. 7-22,\n",
    "December 2020.\n",
    "\n",
    "[4] B. A. Pimentel and A. C. P. L. F. de Carvalho, \"Statistical versus distance-based meta-features for\n",
    "clustering algorithm recommendation using meta-learning,\" in Proc. 2018 International Joint Conference\n",
    "on Neural Networks (IJCNN), 2018, pp. 1-8.\n",
    "\n",
    "[5] B. A. Pimentel and A. C. P. L. F. de Carvalho, \"A Meta-learning approach for recommending the\n",
    "number of clusters for clustering algorithms,\" *Knowledge-Based Systems*, vol. 195, May 2020.\n",
    "\n",
    "[6] A. Jilling and M. Alvarez, \"Optimizing recommendations for clustering algorithms using\n",
    "meta-learning,\" in Proc. 2020 International Joint Conference on Neural Networks (IJCNN), 2020, pp. 1-10.\n",
    "\n",
    "[7] D. Dua and C. Graff, \"UCI Machine Learning Repository,\" *University of California, School of Information and Computer Science*, 2019. [Online]. Available: http://archive.ics.uci.edu/ml. [Accessed: Apr. 22, 2022]\n",
    "\n",
    "[8] “Kaggle.” [Online]. Available: https://www.kaggle.com [Accessed: Jul. 17, 2022.]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
